<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.30.2" />


<title>Dropping T-bombs - Let&#39;s See What Happens When We Take Away The Puppy</title>
<meta property="og:title" content="Dropping T-bombs - Let&#39;s See What Happens When We Take Away The Puppy">



  







<link rel="stylesheet" href="/oliverclark.github.io/css/fonts.css" media="all">
<link rel="stylesheet" href="/oliverclark.github.io/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/oliverclark.github.io/" class="nav-logo">
    <img src="/oliverclark.github.io/images/puppa.JPG"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/oliverclark.github.io/about/">About</a></li>
    
    <li><a href="https://github.com/olijimbo">GitHub</a></li>
    
    <li><a href="https://osf.io/rza9u/">OSF</a></li>
    
    <li><a href="https://twitter.com/PsyTechOli">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">9 min read</span>
    

    <h1 class="article-title">Dropping T-bombs</h1>

    
    <span class="article-date">2018/11/22</span>
    

    <div class="article-content">
      <script src="/oliverclark.github.io/.rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/oliverclark.github.io/.rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="/oliverclark.github.io/.rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="/oliverclark.github.io/.rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/oliverclark.github.io/.rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/oliverclark.github.io/.rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="/oliverclark.github.io/.rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/oliverclark.github.io/.rmarkdown-libs/plotly-main/plotly-latest.min.js"></script>


<p>I have been planning <em>yet another study</em><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>My current experiment is a follow-up to a previous (qualitative) paper that I have now written up. I am using the same task that I used during the interview study, but am measuring self-report likert scales rather than interpreting narratives. My planned study quickly exploded and became hugely complex, so I thought I would take a step back and start by running a short pilot study to investigate whether the manipulation is effective on a single measure before delving into the structural equation modelling nightmare that it has become.</p>
</div>
<div id="the-problem" class="section level2">
<h2>The Problem</h2>
<p>I decided to plan a simple, two group, between subjects pilot study using comparison of means from a questionnaire. I believed that I could get away with a small sample size because of the simplicity of the design. To prepare for this I repeatedly simulated datasets from two populations with the same standard deviation and different means, and checked the false negative rate. I also intended to run equivalence tests on my data, so I used the TOSTER package <span class="citation">(Lakens, 2017)</span>, setting the upper and lower bounds of 0.2 and -0.2 respectively.</p>
<p>I extracted the p values for NHST and both TOST boundaries from each simulation and counted the number of tests that were below an alpha of 0.05<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. I started with a half a standard deviation difference (r = 0.5) and an N of 32. Even though I am very familiar with power and frequentist statistics, I was still surprised that the effect I knew was there was rarely captured by the sample. In fact, the analysis with these parameters only reached 10% power<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> on the Null Hypothesis Significance Tests (NHST), and the Two One Sided Tests were similarly uninformative about the effect.</p>
</div>
<div id="the-method" class="section level2">
<h2>The Method</h2>
<p>To investigate further, I re-ran my simulations altering r and N sequentially to see at what point a minimal power of 80% was reached, as well as checking type 1 error rates (finding an effect when there isn’t one) under conditions of no effect (r = 0) and various meaningless effect sizes (r = 0.05 - 0.02). I also increased the sample size from 10-100 in steps of 10. I ran the simulations 10,000 times for each permutation and tabulated the findings. I found that 80% power was not achieved until r = 0.6 (so a ‘large’ effect size), and N = 90. That is, even with a population effect that was big enough to be seen with the naked eye in a scatterplot, a sample size that is deemed by many researchers (incorrectly) to be sufficient to detect a three way interaction was needed.</p>
<p>Continuing with p-hackerish gumption (I was adamant that I was not going to collect over 100 participants for a pilot study) I decided to switch to bayes factors (<em>bayes-lite</em>). These have potential for misuse in the event of a non-significant p-value, and seemed like a legal move here for critical purposes. In brief, Bayes Factors provide an index of the ratio between the probability of a hypothesis and a null given the data. The boil-in-a-bag rule for BFs (not one to be followed blindly) is that anything over 3 is favourable evidence for H1 and anything less than 0.3 is favourable evidence for H0. BFs were only effective at discriminating effects from non-effects when there was a large sample size and effect size levels effective at finding a known null effect (BF &lt; 0.3), but not effective at finding a known effect. The simulations did not reach 80% power (defined by a BF &gt; 3) until r=0.9 and N = 90. This is important to note in studies claiming support for a null hypothesis. That is, there is no way to determine whether there is a true or a false negative when sample sizes are small!</p>
</div>
<div id="the-simulations" class="section level2">
<h2>The Simulations</h2>
<p>Rather than embed the code on the blog again (I have a detailed description of the process in a previous post), the commented code from the simulations and a set of simulated figures Open Science Framework (<a href="https://osf.io/7c8k3/" class="uri">https://osf.io/7c8k3/</a>). Instead I have presented some plotly interactive graphs for readers to explore the behaviour of P values from NHST, TOST, and BF tests over the two dimensions of effect size and sample size.</p>
<div id="nhst" class="section level3">
<h3>NHST</h3>
<iframe width="900" height="800" frameborder="0" scrolling="no" src="//plot.ly/~OliJ/11.embed">
</iframe>
<div>
<em>NHST power analysis with an independant t-test. Yellow to blue plane is the percentage of observations where p &lt; 0.05</em>
</div>
<p><br></br> When r &gt; 0.2, the NHST plot shows the power (1 - type two error rate * 100) to detect the effect, and when r &lt; 0.2, it shows chance of finding a meaningless or 0 effect (type 1 error, unless rho &lt; 0.2 is a meaningful effect for you). When the effect is around zero, the type one error rate remains constant around around 5% which is commonly seen as an acceptable level. Over the Rho axis you can at best expect power of around 25% when N = 10 but this exponentially increases with the sample size.</p>
</div>
<div id="tost" class="section level3">
<h3>TOST</h3>
<iframe width="900" height="800" frameborder="0" scrolling="no" src="//plot.ly/~OliJ/13.embed">
</iframe>
<div>
<em>Yellow to blue is the rejection of values less than the lower boundary and red to blue is the rejection of values higher than the upper boundary</em>
</div>
<p><br></br> The TOST graph shows the power to determine whether the effect is above -0.2, or below 0.2. At r = 0.9 and N = 90, the tests correctly reject the presence of small effects and absence of large effects, but this decreases substantially as both parameters decrease, meaning that the results become progressively less informative! For more information about powering an equivalence test, see <span class="citation">Lakens (2017)</span>.</p>
</div>
<div id="bayes-factors" class="section level3">
<h3>Bayes Factors</h3>
<iframe width="900" height="800" frameborder="0" scrolling="no" src="//plot.ly/~OliJ/15.embed">
</iframe>
<div>
<em>Blue to red is the percentage of observations where BF &gt; 3, yellow to blue is the percetange of observations where BF &lt; 0.3.</em>
</div>
<p><br></br> The BF_1 and BF_0 plots are mirror opposites, however I have plotted them together to demonstrate that they do not intersect (i.e. false negatives start to become true positives) until r = 0.6 (this is with a sample size of 90). The intersection point forms a curve</p>
</div>
</div>
<div id="so-what" class="section level2">
<h2>So what?</h2>
<p>T-tests are often looked down on as being the easy, exploratory test that require little effort and can be run on small samples to get a quick and dirty answer. I have often said (and heard people say) “It’s a simple study, I’ll just run a t-test on it” with little thought of the implications.</p>
<p>This may not be that surprising. I took a quick look at the closest statistics textbook I have to hand and there is no mention of sample size estimation in the chapter on t-tests. Further, the independent t-test examples provided have small sample sizes (N &lt; 50). Simplified examples are fine for textbooks, but are students being oversold on what t-tests can do?</p>
<p>Further, T-tests are often used on underpowered ANOVA studies for multiple comparisons, and are (perhaps shockingly) often significant even after alpha corrections<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>.</p>
<p>Between subject designs have the potential to be as troublesome as more complex designs, and some appreciation of the sorts of results one can expect from a comparison of means test, when the predicted effect and sample sizes are small is advised. Given the performance of the t-test in finding known effects under conditions of small N and r, researchers should be more critical of significant findings. It is entirely possible (if not more likely) that a significant effect actually represents sampling error and is not representative of the population effect size. This consideration is explained at length by <span class="citation">Button et al. (2013)</span>, who use terms like ‘the winners curse’ and ‘the proteus phenomenon’ to describe instances where inflated effect sizes from poor sampling regress to the much smaller population mean over repeated testing. That is, researchers should be as critical of large effects in small samples as they are of small effects in large samples. As a corollary, in the latter the effect is too small to be meaningful, and in the former the effect may be too large to be true.</p>
<p>Advice from Thom Bagueley on Twitter is to stick to repeated measures experiments as much as possible since the performance of independent t-tests is a feature of the test, especially when there is only one observation per participant.</p>
<div class="figure">
<img src="TB_twitter.png" />

</div>
<p>However, often within subject designs are not possible, particularly when the manipulations are similar, so researchers should be wary of ‘just bashing out’ mean comparison designs without doing a full power analysis. The code above can be used to help with this and can be run in R with few extra packages required (TOST and BayesFactor).</p>
<p>It also turns out that the Bayes Factor may suffer from similar problems, and its performance as a null detector appears to be unsatisfactory at smaller sample sizes. However it is very important to note that in the above simulations I used the default priors. Providing informative priors will likely improve the performance, however these will need to be justified, otherwise you could be accused of SPARKING<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>. One benefit of BFs is that you can sequentially add data, but it’s good to have a ballpark figure of how many you will need.</p>
<p>So should we use t-tests? Yes, of course we should when needed, but they should be treated like any other test! We should also minimise our error rates using good experiment design to minimise other sources of error (using reliable and valid measurements, suitable testing conditions, etc), plan our sample sizes appropriately, consider whether the effect size we are expecting is important enough for the number of participants that would be needed, and if using small samples, don’t just stake your claim on a p &lt; 0.05, replicate!</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-button2013power">
<p>Button, K. S., Ioannidis, J. P., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S., &amp; Munafò, M. R. (2013). Power failure: Why small sample size undermines the reliability of neuroscience. <em>Nature Reviews Neuroscience</em>, <em>14</em>(5), 365–376.</p>
</div>
<div id="ref-Lakens:2017aa">
<p>Lakens, D. (2017). Equivalence tests: A practical primer for t-tests, correlations, and meta-analyses. <em>Social Psychological and Personality Science</em>, <em>1</em>, 1–8. <a href="https://doi.org/10.1177/1948550617697177" class="uri">https://doi.org/10.1177/1948550617697177</a></p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I have so far planned 5 studies throughout my PhD and so far only one has been finished and two have been abandoned (not entirely, but at least from the PhD programme).<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>This method is covered in detail in a previous blog<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>i.e. the percentage of time that a true positive is found<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>The behaviour of t-tests after a one way ANOVA has been discussed elsewhere by <a href="https://alexanderetz.com/2014/09/">Alex Etz</a><a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Specifying Priors After the Results are Known - acronym courtesy of Simine Vazire!<a href="#fnref5">↩</a></p></li>
</ol>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-olijimbo-github-io-oliverclark-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/oliverclark.github.io/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/oliverclark.github.io/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    

    
  </body>
</html>

