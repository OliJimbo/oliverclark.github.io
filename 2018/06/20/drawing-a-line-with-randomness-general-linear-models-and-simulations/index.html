<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.30.2" />


<title>Drawing A Line With Randomness: General Linear Models and Simulations - Let&#39;s See What Happens When We Take Away The Puppy</title>
<meta property="og:title" content="Drawing A Line With Randomness: General Linear Models and Simulations - Let&#39;s See What Happens When We Take Away The Puppy">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/oliverclark.github.io/css/fonts.css" media="all">
<link rel="stylesheet" href="/oliverclark.github.io/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/oliverclark.github.io/" class="nav-logo">
    <img src="/oliverclark.github.io/images/puppa.JPG"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/oliverclark.github.io/about/">About</a></li>
    
    <li><a href="https://github.com/olijimbo">GitHub</a></li>
    
    <li><a href="https://osf.io/rza9u/">OSF</a></li>
    
    <li><a href="https://twitter.com/PsyTechOli">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">11 min read</span>
    

    <h1 class="article-title">Drawing A Line With Randomness: General Linear Models and Simulations</h1>

    
    <span class="article-date">2018/06/20</span>
    

    <div class="article-content">
      <div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#chosing-parameters">Chosing parameters</a></li>
<li><a href="#general-linear-model">General Linear Model</a></li>
<li><a href="#reverse-engineering">Reverse Engineering</a></li>
<li><a href="#running-analyses-welchs-t-test">Running Analyses (Welch’s t Test)</a></li>
<li><a href="#analysis-anova">Analysis (ANOVA)</a></li>
<li><a href="#power-analysis">Power Analysis</a></li>
<li><a href="#brute-force-power-analysis">Brute Force Power Analysis</a></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>I am currently planning a replication and extension of an experimental study that had a count-based dependent variable. The aim is to submit it as a registered report before collecting the data and so I need to have an analysis plan and estimated sample size/stopping rule.</p>
<p>One of the ways that this can be achieved is by simulating the data that you expect to see if your hypothesis is correct. This was suggested by <a href="https://osf.io/sejdw/">Stan Lazic</a> at the Cumberland Lodge Reproducibility Retreat this year and simulation was covered also by <a href="https://osf.io/thpsg/">Dorothy Bishop</a> and <a href="https://osf.io/dk3vr/">Daniel Lakens</a>.</p>
<p>In essence, the point of simulation-based power analysis is to bash out a huge number of ‘replications’ by drawing random numbers that represent the participants in your study. You then count the number of instances where p &lt; 0.05<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<p>There are a number of ways in which this can be achieved, but the one I found most intuitive was reverse engineering the general linear model equation.</p>
<p>The following text and code will cover the following:</p>
<ul>
<li>Deciding on baseline values.</li>
<li>Thinking about how manipulations will affect these values.</li>
<li>Using these two ideals and the equation for a straight line to represent sets of idealised participants.</li>
<li>Run analyses on these imaginary participants.</li>
<li>Repeat the above thousands of times and calculate the power of your study.</li>
<li>Show how sample size affects power.</li>
</ul>
</div>
<div id="chosing-parameters" class="section level2">
<h2>Choosing parameters</h2>
<p>Think of task. It can be anything. If you have choice paralysis let’s say reaction time on an attentional task. If you were to take 1000 participants and test them on this task without changing anything, what would you expect the mean reaction time to be? Let us for now say 900ms <span class="math inline">\(\pm\)</span> 100ms.</p>
<p>How imagine that we take 500 of these participants and load them with some sort of depressant. Let’s say 1000ml of finest Diamond White Cider<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p>
<p>How do you think these participants will respond? Do you think they’ll get slower? Perhaps their reaction time will increase by 200ms, so the mean in this group is 900ms 100ms.</p>
<p>You have just made assumptions that can be represented by the general linear model.</p>
</div>
<div id="general-linear-model" class="section level2">
<h2>General Linear Model</h2>
<p>The form of a general linear model is <span class="math inline">\(y = \alpha + \beta_{1}x_{1} + \beta_{2}x_{2}...+\beta_{n}x_{n} + \epsilon\)</span><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>Beta is the amount of change in y for any value of x. In our above example that is the difference between the two conditions - in this case 200ms. Alpha is the value of y when all beta’s are 0. So 900ms in the group who are not drinking horrible cider. Epsilon accounts for the fact that we are not dealing with machines but random data and is just noise added to the model representing unexplained variation.</p>
<p>You can have as many betas as you like in your equation (or as many as is meaningful) but they are meaningless without something (x) to multiply them by.</p>
<p>X can refer to conditions in an experiment (0 = control (no nasty booze), and 1 = experimental (too much nasty booze)) or some scale that you predict will alter y by a certain amount (average alcohol consumption?).</p>
<p>So applying this to our chosen values above, this can be nicely represented by the model above such that <span class="math inline">\(y_{nothing} = 900 + 200*0 + \epsilon\)</span> or just the base score with error. Making x = 1 means that 200 is added to y.</p>
<p>Thom Bagueley<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> describes the lefthand side of the equation as the random element and the righthand side the systematic element. That is, y depends on both the random attributes of the participant and the random assignment within in the study whereas the righthand side of the equation entirely describes y.</p>
<p>One of the general goals of regression analysis is to estimate each <span class="math inline">\(\beta_i\)</span> given <span class="math inline">\(X_i\)</span> and <span class="math inline">\(y_i\)</span><a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>.</p>
</div>
<div id="reverse-engineering" class="section level2">
<h2>Reverse Engineering</h2>
<p>When reading around how to simulate data I stumbled across a <a href="https://stats.stackexchange.com/questions/115748/simulate-data-for-2-x-2-anova-wi">StackOverflow post</a> that suggests reverse engineering this equation to simulate the data. That is, rather than estimating <span class="math inline">\(\beta_{i}\)</span> given y and x, you calculate y given a set of <span class="math inline">\(\beta\)</span>’s and x and adding some random error to it.</p>
<p>This approach helped a lot, since before that I was relying on example of full factorial designs using matrix multiplication^[I had no idea what this was or how to do it - <a href="http://www.sosmath.com/matrix/matrix2/matrix2.html">this link was really valuable if you are interested</a>.</p>
<p>Lets do an example in R:</p>
<pre class="r"><code>#set parameters

a = 900 # Value at intercept when X = 0
b1 = 200 # The about that X is multiplied by

#create a vector of conditions

x &lt;- c(1) #X is one here - if it was 0 then it would be an intercept only model/control condition

#calculate a single observation of y

y &lt;- a + b1*x[1]</code></pre>
<p>This creates a value of y that is 900 + 200 = 1100ms. Of course, because we’re dealing with randomness, the parameters will not be perfectly reflected when collecting data from participants, so we add some random noise to y. This is taken from a normal distribution with a mean of 0 and a standard deviations of 100ms:</p>
<pre class="r"><code>y &lt;- a + b1*x[1] + rnorm(1,0,100)
y</code></pre>
<pre><code>## [1] 1002.778</code></pre>
<p>In this observation our simulated participant was only exposed to one condition.</p>
<p>We can expand this to an entire set of participants easily by just changing a few of the lines that we already have:</p>
<pre class="r"><code>a = 900
b1 = 200

# create a vector of conditions for 100 participants

x &lt;- rep(c(1,0),50)

y &lt;- a + b1 * x + rnorm (100,0,100) #compute y for 100 people with normal error

paste(&quot;Mean of Control Group =&quot;, round(mean(y[x==0]), digits = 2))</code></pre>
<pre><code>## [1] &quot;Mean of Control Group = 905.78&quot;</code></pre>
<pre class="r"><code>paste(&quot;Mean of Cider Group =&quot;, round(mean(y[x==1]), digits = 2))</code></pre>
<pre><code>## [1] &quot;Mean of Cider Group = 1125.37&quot;</code></pre>
<p>You can explore what is going on here by typing the individual variable names into the R console.</p>
</div>
<div id="running-analyses-welchs-t-test" class="section level2">
<h2>Running Analyses (Welch’s t Test)</h2>
<p>We now have 100 participants drawn from the control condition (intercept only) and an experimental condition (mean difference = 2).</p>
<p>To keep things neat, we can create a dataframe to store all of the variables in one place:</p>
<pre class="r"><code>dat &lt;- as.data.frame(cbind(y,x,pp = 1:100))</code></pre>
<p>We can then run a t-test on this equation to see if there is a significant difference:</p>
<pre class="r"><code>t.test(y ~ x, data = dat)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  y by x
## t = -11.959, df = 91.467, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -256.0534 -183.1135
## sample estimates:
## mean in group 0 mean in group 1 
##        905.7823       1125.3658</code></pre>
<p>As you can see the mean in group 0 is close to 900 and the mean in group1 is close to 200 which is what we coded our betas as.</p>
</div>
<div id="analysis-anova" class="section level2">
<h2>Running Analyses (ANOVA)</h2>
<p>This can easily be extended to multiple variables and then analysed using Analysis of Variance. Perhaps we give a second group a similar dose of Buckfast<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>:</p>
<pre class="r"><code>a = 900
b1 = 200
b2 = 500

#three groups, 33 a piece

grp1 &lt;- c(rep(0,33),rep(1,33),rep(0,33))
grp2 &lt;- c(rep(0,66),rep(1,33))

y &lt;- a + b1*grp1 + b2 * grp2 + rnorm(99,0,100)

dat &lt;- as.data.frame(cbind(y,grp1,grp2,pp = 1:99))

summary(aov(y ~ grp1 * grp2, data = dat))</code></pre>
<pre><code>##             Df  Sum Sq Mean Sq F value  Pr(&gt;F)    
## grp1         1  105080  105080   9.836 0.00227 ** 
## grp2         1 4068777 4068777 380.844 &lt; 2e-16 ***
## Residuals   96 1025624   10684                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>If you run a regression analysis on the same data you will find the values that we set are retrieved:</p>
<pre class="r"><code>summary(lm(y~grp1 * grp2, data = dat))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ grp1 * grp2, data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -246.260  -64.999    3.171   76.479  201.541 
## 
## Coefficients: (1 not defined because of singularities)
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   928.11      17.99  51.582  &lt; 2e-16 ***
## grp1          179.18      25.45   7.042 2.85e-10 ***
## grp2          496.58      25.45  19.515  &lt; 2e-16 ***
## grp1:grp2         NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 103.4 on 96 degrees of freedom
## Multiple R-squared:  0.8027, Adjusted R-squared:  0.7986 
## F-statistic: 195.3 on 2 and 96 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="power-analysis" class="section level2">
<h2>Power Analysis</h2>
<p>I wont go too far into what Power Analysis is, other than that it tells you the probability of you finding an effect, if there is one, given three parameters:</p>
<ol style="list-style-type: decimal">
<li>Sample size.</li>
<li>Effect Size.</li>
<li>Type 1 error rate (how willing you are to find an effect when there isn’t one).</li>
</ol>
<p>Typically it is used for sample size estimation using software like <a href="http://gpower.hhu.de">G*Power</a> and you would input the smallest effect size that you would deem to be meaningful, your type one error rate (usually 0.05, but see the link above to find out how to justify your this value) and how willing you are to miss an effect if there is one (type two error rate).</p>
<p>Another way to run a power analysis that doesn’t necessarily require you to know how to compute Cohen’s <span class="math inline">\(F^2\)</span> is to run simulations as we have above. This way we have to think about what change from the intercept value we think is meaningful. It also means that we have to know how to analyse our data before we collect it because as we have seen, the same model (GLM) is used to both generate fake samples and analyse the real data.</p>
</div>
<div id="brute-force-power-analysis" class="section level2">
<h2>Brute Force Power Analysis</h2>
<p>Using the methods above, you can run the simulation thousands of times and find out what power your model has:</p>
<pre class="r"><code>#Create an empty dataframe
P &lt;- data.frame()
#Set a loop to repeat the code in Curley brackets ten thousand times

for(i in 1:1e4){

a = 900
b1 = 200
b2 = 500

#three groups, 33 a piece

grp1 &lt;- c(rep(0,33),rep(1,33),rep(0,33))
grp2 &lt;- c(rep(0,66),rep(1,33))

y &lt;- a + b1*grp1 + b2 * grp2 + rnorm(99,0,100)

dat &lt;- as.data.frame(cbind(y,grp1,grp2,pp = 1:99))

#The right side of this looks horrible but it is just an index to the p values in the summary object

P &lt;- rbind(P,P&lt;-summary(aov(y ~ grp1 * grp2, data = dat))[[1]][[&quot;Pr(&gt;F)&quot;]][[1]][[1]])

}
#Calculate the power
Power &lt;- length(P[P&lt;0.05])/1e4*100</code></pre>
<p>So given the provided mean differences, sample size and number of groups, this simulated study had a power of 64.54.</p>
<p>To explore this further you can create a function and pass different sample sizes into the model. So, below we start with 9 participants and then gradually work up to over 300:</p>
<pre class="r"><code>Power_Calc &lt;- function(n){
a = 900
b1 = 200
b2 = 500

#three groups, n/3 a piece

grp1 &lt;- c(rep(0,n/3),rep(1,n/3),rep(0,n/3))
grp2 &lt;- c(rep(0,(n/3*2)),rep(1,n/3))

y &lt;- a + b1*grp1 + b2 * grp2 + rnorm(n,0,100)

dat &lt;- as.data.frame(cbind(y,grp1,grp2,pp = 1:n))

P &lt;- summary(aov(y ~ grp1 * grp2, data = dat))[[1]][[&quot;Pr(&gt;F)&quot;]][[1]][[1]]

return(P)
}

Power &lt;- data.frame() # Empty dataframe
n = 9 # starting value of 9 participants)

for(i in 1:100){
Power &lt;- rbind(Power,cbind(Power_Calc(n),n))
n = n+3 #Keeping equal groups
}

colnames(Power) &lt;- c(&quot;P_Value&quot;, &quot;Participants&quot;)
plot(P_Value~Participants, data = Power)</code></pre>
<p><img src="/oliverclark.github.io/post/2018-06-20-drawing-a-line-with-randomness-general-linear-models-and-simulations_files/figure-html/Power-2-1.svg" width="576" /></p>
<p>We can then plot the data, as above, to show what p-values you can expect for different sample sizes. It is also a good demonstration of how even though we know there is a mean difference of between 200-500, you can still get values way above 0.05 even with a lot of participants. Try running the code a few times and playing with the different beta values.</p>
<p>This is by no means the best or most efficient method for running power analysis using simulation (I would suggest working through the documents on the OSF page referenced above to see some more elegant methods), but hopefully it seems intuitive enough to get started.</p>
<p>In the following weeks I intend to expand on this method, demonstrating how you can use it to run generalised linear models such as count and logistic regression studies. Since I am quite in to Bayesian statistics and the purpose of me doing this analysis was to plan a Bayesian study, I may throw in some of that too.</p>
<p>Thanks for reading!</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://osf.io/by2kc/">or whichever alpha you choose</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>This is what many UK teenagers first discover hangovers with^ [My first draft of this mentioned White Lightening which was notorious when I was at school - but I have since found that it was discontinued in 2009]<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>I am using Greek letters below because this is the terminology used in regression equations.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>I recently bought his book Serious Stats and it is really accessible given the content - definitely one to buy if you’re ready to get serious with your statistics!<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>oughta!<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>this one is more for the students and those north of Hadrian’s wall<a href="#fnref6">↩</a></p></li>
</ol>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-olijimbo-github-io-oliverclark-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/oliverclark.github.io/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/oliverclark.github.io/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/oliverclark.github.io/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

