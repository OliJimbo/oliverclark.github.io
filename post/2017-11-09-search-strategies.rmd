---
title: Search Strategies
author: Oliver
date: '2017-11-09'
slug: search-strategies
categories: [meta-analysis, systematic review, PhD]
tags: [meta-analysis, systematic review, PhD, Search Criteria]
---

Today's lesson is that writing systematic search terms is hard.

That is a bit of an oversimplification, which is the topic of this blog.

It has actually taken me two days to figure this out.

I have read a number of meta-analysis papers, have reproduced one and am currently embarking on my very own. The methods sections of these papers are usually very zen:
```
The search terms were set
"This phrase" AND (this word OR that)
Two hundred Retrieved
```

Naturally I totally bought this and excitedly started my search using the draft search strategy I had produced for my PRISMA statement. I soon found that it is not such a straight forward process. I was met with syntax inconsistencies, Python-like errors due to mismatched parentheses and quote-marks which were lost during copying and pasting, and vast differences in the number of papers retrieved using the same terms (between databases, and in one case within!^[It turned out to be a parentheses issue, but for a moment there....]. 

This was not nearly as clear cut as the literature would have me believe!

I started to think that this was due to a failure in planning on my part. I gave myself the benefit of the doubt and revisited a published meta-analysis.  I copied and ran their search terms through some of the databases they queried (not all were available to me).

Journals    | Citations Retrived
----------------|-------------------
ProQuest Nursing|20135
CINAHL     |53
Zetoc      |0
Web of Knowledge|215
Proquest Theses |2731
PsycArticles  |7
PsycInfo    |153
OVID Journals  |480
-------------------------------------

The authors reported finding just over 320 papers before selection.

Looking at the number of retrieved citations it looks as though the authors had a similar problem to me but just didn't report it!

This isn't a debunking attempt which is why I am not naming the paper which is very thorough with preferred reporting items provided and at least one replication/adaptation that I know of. 

I also understand why the process of developing search terms isn't described in detail in journals. It would be incredibly boring to read!

But in cases like ProQuest Nursing above there is a very low probability that the authors sifted through 20,000 entries.

#Upon Reflection
In an unusual act of reflective practice I considered what steps I went through to develop my search terms over the past two days (and well into last night) to see if I could find some method in the madness.

They were broadly:

- Training Phase
- Refinement Phase
- Adaptation Phase

This is by no means something that I planned, but more of an attempt to understand a hazy and chaotic process in terms of a model.

##Training
This was the primordial mash. It involved throwing in any relevant search terms including long complex terms with lots of phrases and brackets. A bit like when you get a new guitar pedal or an avatar creation platform on a video game and just crank up all of the parameters just to see how ridiculous you can make it sound/look.

I found that through this process I learned what sort of terms and combinations of functions restrict the yield and open the flood gates letting any old nonsense through. I found that this gave me a good idea of the boundaries of my primary database, in this case Scopus, and let me make more informed choices in later iterations. Gradually the number and complexity of the terms reduced.

An example of one of my training searches is:

```
(TITLE-ABS-KEY ((avatar AND ("Proteus Effect" ) OR 
(self AND avatar ) OR (virtual AND self OR doppelg√§nger ) 
OR ("Video game" OR video-game OR "Virtual reality" OR "Second life" 
OR wii OR Kinect OR "Playstation Move" ) ) ) AND TITLE-ABS-KEY (health* 
OR diet OR exercise OR alcohol OR "ultra violet" OR smoking OR sex* 
OR substance OR addiction OR (body OR image OR dissatisfaction ) 
AND (behavio* AND (change OR intention OR attitude ) OR intervention ) ) 
AND NOT TITLE-ABS-KEY ("Stem Cell" OR bacter* OR phobi* OR anxiety 
OR anxious OR psychiat* OR "mentalhealth" OR "e-mental health" OR rehabilitation ) )
```

##Refinement
This involved trying to use fewer terms to get the best looking yield. By best looking I mean nothing absurdly small or obscenely large. I didn't look at the titles of the papers much initially, but eventually started checking keywords to see if anything could be added to the terms. I tried to avoid using the AND NOT function as much as possible. I also took a look at some previous published search strategies to see what published ones look like and tried to adapt mine to be more succinct. An example was:
```
(TITLE-ABS-KEY (avatar OR "proteus effect" OR "virtual self-representation" 
OR "virtual self-modelling" OR (avatar W/3 (self OR similar OR relevant))) 
AND TITLE-ABS-KEY (health* OR "body image") AND TITLE-ABS-KEY (beliefs OR attitudes 
OR prevent OR chang* OR intervention*)) 
```

##Adaptation
Involved porting the search terms over to other databases. This is often just a matter of copying and pasting, but other times the syntax is different, for example searching for two words and allowing up 3 words between them in Scopus is achieved with Word_1 W/3 Word2. However, in PsycArticles this means that Word_1 must be before Word_2. You have to use N3 in PsychArticles, or NEAR/3 in Web of Science.
Further, I found that a predominantly medical platform like PubMed retrieved more mental health articles so I needed to use the NOT statement e.g.:

```
(((Avatar OR "proteus effect" OR "virtual self-representation" 
OR "virtual self-modelling" OR (avatar W/3 (self OR similar OR relevant)))) 
AND (Health* OR "body image")) AND (Beliefs OR attitudes OR prevent OR chang* 
OR intervention*) NOT "Mental health"
```

#Conclusion
In my mind as I reflected on several hours of grilling the databases, I started to see the process as being like the Bayesian warmup phase in HMC simulation. To begin with you are all over the place, you retrieve tens of thousands (in one case yesterday millions) of papers, the majority of which are entirely useless for your research question, and slowly drill down until you find the space you need to occupy to sample your data from (in this case studies).

I wanted to find a home for this experience before I start my write up and my method becomes haiku too.
