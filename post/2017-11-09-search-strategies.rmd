---
title: Search Strategies
author: Oliver
date: '2017-11-09'
slug: search-strategies
categories: [meta-analysis, systematic review, PhD]
tags: [meta-analysis, systematic review, PhD, Search Criteria]
---

Today's lesson was that writing systematic search terms is hard.

That is an oversimplification, which is the topic of this blog.
It has actually taken me two days to figure this out.

I have read a number of meta-analysis papers, have reproduced one and am currently embarking on my very own.  The methods sections of these papers are usually very zen:
```
The search terms were set
"This phrase" AND (this word OR that*)
Two hundred Retrieved
```

Naturally I swallowed this down and excitedly started my search using the draft search strategy I had produced for my PRISMA statement and found that it is not a straight forward process.  I was met with syntax inconsistencies, Python-like errors due to mismatched parentheses and quote-marks which were lost during copying and pasting, and vast differences in the number of papers retrieved using the same terms (between databases, and within in one case!^[It turned out to be a parentheses issue, but for a moment there....].  

This was not nearly as clear cut as the literature would have me believe!

I started to think that this was all down to me not planning properly.  To give myself the benefit of the doubt I revisited a published meta-analysis and ran their search terms through some of the databases they queried (not all were available to me).

Journals        | Citations Retrived
----------------|-------------------
ProQuest Nursing|20135
CINAHL          |53
Zetoc           |0
Web of Knowledge|215
Proquest Theses |2731
PsycArticles    |7
PsycInfo        |153
OVID Journals   |480
-------------------------------------

The authors reported finding just over 320 papers before selection.

Looking at the number of retrieved citations, the authors probably had a similar problem to me, but just didn't report it!

This isn't a debunking attempt or anything like that, which is why I am not naming the paper. It is a very thorough meta-analysis with preferred reporting items provided and at least one replication/adaptation that I know of. 

I also understand why the process of developing search terms isn't described in detail in journals.  It would be incredibly boring to read!

But in cases like ProQuest Nursing there is a very low probability that the authors sifted through 20,000 entries.

#Upon Reflection
In an unusual act of reflective practice, I considered what steps I went through to develop my search terms over the past two days (and well into last  night) to see if I could find some method in the madness.  They were broadly:

*Training Phase
*Refinement Phase
*Adaptation Phase

This is by no means something that I planned, more of an attempt to understand a hazy and chaotic process in terms of a model.

##Training
This was the primordial mash.  It involved throwing in any relevant search terms, including long complex terms with lots of phrases and brackets.  A bit like when you get a new guitar pedal, or an avatar creation platform on a video game, you might just crank up all of the parameters just to see how ridiculous you can make it sound/look.

I found that through this process I learned what sort of terms and combinations of functions restrict the yield and open the flood gates letting any old nonsense through.  I found that this gave me a good idea of the boundaries of my primary database, in this case Scopus and let me make more informed choices in later iterations.  Gradually the number and complexity of the terms reduced.

An example training search is was:

```
( TITLE-ABS-KEY ( ( avatar  AND  ( "Proteus Effect" )  OR  
( self  AND  avatar )  OR  ( virtual  AND  self  OR  doppelg√§nger )  
OR  ( "Video game"  OR  video-game  OR  "Virtual reality"  OR  "Second life"  
OR  wii  OR  Kinect  OR  "Playstation Move" ) ) )  AND  TITLE-ABS-KEY ( health*  
OR  diet  OR  exercise  OR  alcohol  OR  "ultra violet"  OR  smoking  OR  sex* 
OR substance  OR  addiction  OR  ( body  OR  image  OR  dissatisfaction )  
AND  ( behavio*  AND  ( change  OR  intention  OR  attitude )  OR  intervention ) )  
AND NOT  TITLE-ABS-KEY ( "Stem Cell"  OR  bacter*  OR  phobi*  OR  anxiety  
OR  anxious  OR  psychiat*  OR  "mentalhealth"  OR  "e-mental health"  OR  rehabilitation ) )
```

##Refinement
This involved trying to use fewer terms to get the best looking yield.  By best looking I mean nothing absurdly small or obscenely high.  I didn't look at the titles of the papers much, and tried to avoid using the AND NOT function as much as possible.  I also took a look at some previous search strategies to see what published ones look like and tried to adapt mine to be more succinct.  An example was:
```
( TITLE-ABS-KEY ( avatar OR  "proteus effect"  OR  "virtual self-representation"  OR  "virtual self-modelling"  OR  ( avatar  W/3  ( self  OR  similar  OR  relevant ) ) )  AND  TITLE-ABS-KEY ( health*  OR  "body image" )  AND  TITLE-ABS-KEY ( beliefs  OR  attitudes  OR  prevent  OR  chang*  OR  intervention* ) ) 
```

##Adaptation
Involved porting the search terms over to other databases.  This is often just a matter of copying and pasting, but other times the syntax is different, for example searching for two words and allowing up 3 words between them in Scopus is achieved with Word_1 W/3 Word2.  However, in PsycArticles this means that Word_1 must be before Word_2.  You have to use N3 in PsychArticles, or NEAR/3 in Web of Science.
Further, I found that a predominantly medical platform like PubMed retrieved more mental health articles so I needed to use the NOT statement e.g.:

```
(((Avatar OR "proteus effect" OR "virtual self-representation" OR "virtual self-modelling" OR (avatar W/3 (self OR similar OR relevant)))) AND (Health* OR "body image")) AND (Beliefs OR attitudes OR prevent OR chang* OR intervention*) NOT "Mental health"
```

#Conclusion
In my mind as I reflected on several hours of grilling the databases, I started to see the process as being like the Bayesian warmup phase in HMC simulation.  To begin with you are all over the place, you retrieve tens of thousands (in one case yesterday millions) of papers, the majority of which are entirely useless for your research question, and slowly drill down until you find the space you need to occupy to sample your data from (in this case studies).

I wanted to find a home for this experience before I start my write up and my method becomes haiku too.
